{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c43497",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# spacy.require_gpu()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01men_core_web_sm\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, string\n",
    "# spacy.require_gpu()\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from plotly.offline import plot\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e305ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the display properties of pandas to max\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c64ca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data . The data is in JSON format and we need to convert it to a dataframe.\n",
    "# Opening JSON file \n",
    "file = open('complaints-2021-05-14_08_16.json')\n",
    "data = json.load(file)\n",
    "df=pd.json_normalize(data)\n",
    "\n",
    "# Data Preparation\n",
    "# Inspect the dataframe to understand the given data.\n",
    "df.info()\n",
    "\n",
    "#Assign new column names\n",
    "df.rename(columns={'_index':'index',\n",
    "  '_type':'type',\n",
    "  '_id':'id',\n",
    "  '_score':'score',\n",
    "  '_source.tags':'tags',\n",
    "  '_source.zip_code':'',\n",
    " '_source.complaint_id':'complaint_id',\n",
    " '_source.issue':'issue',\n",
    " '_source.date_received':'date_received',\n",
    " '_source.state':'state',\n",
    " '_source.consumer_disputed':'consumer_disputed',\n",
    " '_source.product':'product',\n",
    " '_source.company_response':'company_response',\n",
    " '_source.company':'company',\n",
    " '_source.submitted_via':'submitted_via',\n",
    " '_source.date_sent_to_company':'date_sent_to_company',\n",
    " '_source.company_public_response':'company_public_response',\n",
    " '_source.sub_product':'sub_product',\n",
    " '_source.timely':'timely',\n",
    " '_source.complaint_what_happened':'complaint_what_happened',\n",
    " '_source.sub_issue':'sub_issue',\n",
    " '_source.consumer_consent_provided':'consumer_consent_provided'},inplace=True)\n",
    "#Assign nan in place of blanks in the complaint_what_happened column\n",
    "df['complaint_what_happened'].replace('', np.nan, inplace=True)\n",
    "# Null values count after replacing blanks with nan\n",
    "df['complaint_what_happened'].isnull().sum()\n",
    "\n",
    "#Remove all rows where complaint_what_happened column is nan\n",
    "df.dropna(subset=['complaint_what_happened'],inplace=True)\n",
    "\n",
    "\n",
    "# Prepare the text for topic modeling.Once you have removed all the blank complaints, you need to:\n",
    "#Make the text lowercase\n",
    "#Remove text in square brackets\n",
    "#Remove punctuation\n",
    "#Remove words containing numbers\n",
    "#Once you have done these cleaning operations you need to perform the following:\n",
    "#Lemmatize the texts\n",
    "# Extract the POS tags of the lemmatized text and remove all the words which have tags other than NN[tag == \"NN\"].\n",
    "\n",
    "# Write your function here to clean the text and remove all the unnecessary elements.\n",
    "def clean_text(text):\n",
    "  text=text.lower()  #convert to lower case\n",
    "  text=re.sub(r'^\\[[\\w\\s]\\]+$',' ',text) #Remove text in square brackets\n",
    "  text=re.sub(r'[^\\w\\s]',' ',text) #Remove punctuation\n",
    "  text=re.sub(r'^[a-zA-Z]\\d+\\w*$',' ',text) #Remove words with numbers\n",
    "  return text\n",
    "#Write your function to Lemmatize the texts\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "def lemmatization(texts):\n",
    "    lemma_sentences = []\n",
    "    for doc in tqdm(nlp.pipe(texts)):\n",
    "        sent = [token.lemma_ for token in doc if token.text not in set(stopwords)]\n",
    "        lemma_sentences.append(' '.join(sent))\n",
    "    return lemma_sentences\n",
    "#Create a dataframe('df_clean') that will have only the complaints and the lemmatized complaints \n",
    "df_clean = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Clean text columns\n",
    "df_clean['complaint_what_happened'] = df['complaint_what_happened'].progress_apply(lambda x: clean_text(x))\n",
    "\n",
    "# adding category and sub_category columns to the dataframe for better topic identification\n",
    "df_clean['category'] = df['product']\n",
    "df_clean['sub_category'] = df['sub_product']\n",
    "#Write your function to extract the POS tags only for NN\n",
    "def extract_pos_tags(texts):\n",
    "    pos_sentences = []\n",
    "    for doc in tqdm(nlp.pipe(texts)):\n",
    "        sent = [token.text for token in doc if token.tag_ == 'NN']\n",
    "        pos_sentences.append(' '.join(sent))\n",
    "    return pos_sentences\n",
    "\n",
    "df_clean[\"complaint_POS_removed\"] = extract_pos_tags(df_clean['complaint_what_happened_lemmatized'])\n",
    "\n",
    "#The clean dataframe should now contain the raw complaint, lemmatized complaint and the complaint after removing POS tags.\n",
    "df_clean.head()\n",
    "\n",
    "\n",
    "#Exploratory data analysis to get familiar with the data.\n",
    "#Write the code in this task to perform the following:\n",
    "\n",
    "#Visualise the data according to the 'Complaint' character length\n",
    "#Using a word cloud find the top 40 words by frequency among all the articles after processing the text\n",
    "#Find the top unigrams,bigrams and trigrams by frequency among all the complaints after processing the text. â€˜\n",
    "# Write your code here to visualise the data according to the 'Complaint' character length\n",
    "df_clean['complaint_length'] = df_clean['complaint_what_happened'].str.len()\n",
    "df_clean['complaint_what_happened_lemmatized_length'] = df_clean['complaint_what_happened_lemmatized'].str.len()\n",
    "df_clean['complaint_POS_removed_length'] = df_clean['complaint_POS_removed'].str.len()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=df_clean['complaint_length'], name='Complaint'))\n",
    "fig.add_trace(go.Histogram(x=df_clean['complaint_what_happened_lemmatized_length'], name='Complaint Lemmatized'))\n",
    "fig.add_trace(go.Histogram(x=df_clean['complaint_POS_removed_length'], name='Complaint POS Removed'))\n",
    "fig.update_layout(barmode='overlay', title='Complaint Character Length', xaxis_title='Character Length', yaxis_title='Count')\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()\n",
    "\n",
    "# Find the top unigrams,bigrams and trigrams by frequency among all the complaints after processing the text.\n",
    "\n",
    "# function to get the specified top n-grams\n",
    "def get_top_n_words(corpus, n=None,count=None):\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:count]\n",
    "\n",
    "#Print the top 10 words in the unigram frequency and plot the same using a bar graph\n",
    "unigram = get_top_n_words(df_clean['Complaint_clean'], 1,10)\n",
    "for word, freq in unigram:\n",
    "    print(word, freq)\n",
    "px.bar(x=[word for word, freq in unigram], y=[freq for word, freq in unigram], title='Top 10 Unigrams')\n",
    "\n",
    "#Print the top 10 words in the bigram frequency and plot the same using a bar graph\n",
    "bigram = get_top_n_words(df_clean['Complaint_clean'], 2,10)\n",
    "for word, freq in bigram:\n",
    "    print(word, freq)\n",
    "px.bar(x=[word for word, freq in bigram], y=[freq for word, freq in bigram], title='Top 10 Bigrams')\n",
    "\n",
    "#Print the top 10 words in the trigram frequency and plot the same using a bar graph\n",
    "trigram = get_top_n_words(df_clean['Complaint_clean'], 3,10)\n",
    "for word, freq in trigram:\n",
    "    print(word, freq)\n",
    "px.bar(x=[word for word, freq in trigram], y=[freq for word, freq in trigram], title='Top 10 Trigram')\n",
    "\n",
    "#The personal details of customer has been masked in the dataset with xxxx. Let's remove the masked text as this will be of no use for our analysis\n",
    "df_clean['Complaint_clean'] = df_clean['Complaint_clean'].str.replace('xxxx','')\n",
    "#All masked texts has been removed\n",
    "df_clean.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656fb882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a document term matrix using fit_transform\n",
    "# The contents of a document term matrix are tuples of (complaint_id,token_id) tf-idf score: The tuples that are not there have a tf-idf score of 0\n",
    "\n",
    "#Write your code here to create the Document Term Matrix by transforming the complaints column present in df_clean.\n",
    "tfidf=tf_idf_vec.fit_transform(df_clean['Complaint_clean'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a0cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Topic Modelling \n",
    "\n",
    "#Load your nmf_model with the n_components i.e 5\n",
    "num_topics = 5\n",
    "\n",
    "#keep the random_state =40\n",
    "nmf_model = NMF(n_components=num_topics, random_state=40)\n",
    "nmf_model.fit(tfidf)\n",
    "len(tf_idf_vec.get_feature_names_out())\n",
    "\n",
    "#Print the Top15 words for each of the topics\n",
    "for index, topic in enumerate(nmf_model.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index} with tf-idf score')\n",
    "    print([tf_idf_vec.get_feature_names_out()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')\n",
    "    \n",
    " #Create the best topic for each complaint in terms of integer value 0,1,2,3 & 4\n",
    "topic_values = nmf_model.transform(tfidf)\n",
    "topic_values.argmax(axis=1)\n",
    "\n",
    "array([0, 1, 3, ..., 3, 1, 2], dtype=int64)\n",
    "#Assign the best topic to each of the cmplaints in Topic Column\n",
    "df_clean['Topic'] = topic_values.argmax(axis=1)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e9fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dictionary of Topic names and Topics\n",
    "\n",
    "Topic_names = {\n",
    "    0: 'Bank Account services',\n",
    "    1: 'Credit card or prepaid card',\n",
    "    2: 'Others',\n",
    "    3: 'Theft/Dispute Reporting',\n",
    "    4: 'Mortgage/Loan'\n",
    "}\n",
    "#Replace Topics with Topic Names\n",
    "df_clean['Topic_category'] = df_clean['Topic'].map(Topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b2e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep the columns\"complaint_what_happened\" & \"Topic\" only in the new dataframe --> training_data\n",
    "training_data=df_clean[['complaint_what_happened','Topic']]\n",
    "training_data.head()\n",
    "\n",
    "#Write your code to get the Vector count\n",
    "X_train_counts=count_vect.fit_transform(training_data['complaint_what_happened'])\n",
    "#Write your code here to transform the word vector to tf-idf\n",
    "tfidf_transformer=TfidfTransformer()\n",
    "X_train_tf=tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Checking for class imbalance\n",
    "px.bar(x=training_data['Topic'].value_counts().index, y=training_data['Topic'].value_counts().values/max(training_data['Topic'].value_counts().values), title='Class Imbalance')\n",
    "\n",
    "# importing libraries required for model building and evaluation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold,GridSearchCV,train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,precision_score,recall_score,f1_score,classification_report\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "# Prepare the training and test data\n",
    "train_X, test_X, train_y, test_y = train_test_split(X_train_tf, training_data['Topic'], test_size=0.2, random_state=40)\n",
    "# function to evaluate the model and display the results\n",
    "def eval_model(y_test,y_pred,y_pred_proba,type='Training'):\n",
    "    print(type,'results')\n",
    "    print('Accuracy: ', accuracy_score(y_test,y_pred).round(2))\n",
    "    print('Precision: ', precision_score(y_test,y_pred,average='weighted').round(2))\n",
    "    print('Recall: ', recall_score(y_test,y_pred,average='weighted').round(2))\n",
    "    print('F1 Score: ', f1_score(y_test,y_pred,average='weighted').round(2))\n",
    "    print('ROC AUC Score: ', roc_auc_score(y_test,y_pred_proba,average='weighted',multi_class='ovr').round(2))\n",
    "    print('Classification Report: ', classification_report(y_test,y_pred))\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=training_data['Topic'].unique())\n",
    "    disp.plot()\n",
    "# function to grid search the best parameters for the model\n",
    "def run_model(model,param_grid):\n",
    "    cv=StratifiedKFold(n_splits=5,shuffle=True,random_state=40)\n",
    "    grid=GridSearchCV(model,param_grid={},cv=cv,scoring='f1_weighted',verbose=1,n_jobs=-1)\n",
    "    grid.fit(train_X,train_y)\n",
    "    return grid.best_estimator_\n",
    "1. Logistic Regression\n",
    "#running and evaluating the Logistic Regression model\n",
    "params = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': [100, 200, 300, 500, 1000],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "model=run_model(LogisticRegression(),params)\n",
    "eval_model(train_y,model.predict(train_X),model.predict_proba(train_X),type='Training')\n",
    "eval_model(test_y,model.predict(test_X),model.predict_proba(test_X),type='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fb9eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running and evaluating the Decision Tree model\n",
    "params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 2, 4, 6, 8, 10],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8, 10],\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2']\n",
    "}\n",
    "model=run_model(DecisionTreeClassifier(),params)\n",
    "eval_model(train_y,model.predict(train_X),model.predict_proba(train_X),type='Training')\n",
    "eval_model(test_y,model.predict(test_X),model.predict_proba(test_X),type='Test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da3da69e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Random Forest \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#running and evaluating the Random Forest model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m500\u001b[39m],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcriterion\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbootstrap\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m]\n\u001b[0;32m     12\u001b[0m }\n\u001b[1;32m---> 13\u001b[0m model\u001b[38;5;241m=\u001b[39mrun_model(RandomForestClassifier(),params)\n\u001b[0;32m     14\u001b[0m eval_model(train_y,model\u001b[38;5;241m.\u001b[39mpredict(train_X),model\u001b[38;5;241m.\u001b[39mpredict_proba(train_X),\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m eval_model(test_y,model\u001b[38;5;241m.\u001b[39mpredict(test_X),model\u001b[38;5;241m.\u001b[39mpredict_proba(test_X),\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Random Forest \n",
    "\n",
    "#running and evaluating the Random Forest model\n",
    "params = {\n",
    "    'n_estimators': [10, 50, 100, 200, 500],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 2, 4, 6, 8, 10],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8, 10],\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "model=run_model(RandomForestClassifier(),params)\n",
    "eval_model(train_y,model.predict(train_X),model.predict_proba(train_X),type='Training')\n",
    "eval_model(test_y,model.predict(test_X),model.predict_proba(test_X),type='Test')\n",
    "\n",
    "\n",
    "#running and evaluating the Gaussian Naive Bayes model\n",
    "params = {\n",
    "    'alpha': [0.1, 0.5, 1, 2, 5],\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "model=run_model(MultinomialNB(),params)\n",
    "eval_model(train_y,model.predict(train_X),model.predict_proba(train_X),type='Training')\n",
    "eval_model(test_y,model.predict(test_X),model.predict_proba(test_X),type='Test')\n",
    "\n",
    "#running and evaluating the XGBoost model\n",
    "params = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'gamma': [0, 0.5, 1],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.5, 0.8, 1],\n",
    "    'colsample_bytree': [0.5, 0.8, 1]\n",
    "}\n",
    "model=run_model(XGBClassifier(),params)\n",
    "eval_model(train_y,model.predict(train_X),model.predict_proba(train_X),type='Training')\n",
    "eval_model(test_y,model.predict(test_X),model.predict_proba(test_X),type='Test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb8557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the best model on the Custom Text\n",
    "# We will use the XGBoost model as it has the best performance\n",
    "df_complaints = pd.DataFrame({'complaints': [\"I can not get from chase who services my mortgage, who owns it and who has original loan docs\", \n",
    "                                  \"The bill amount of my credit card was debited twice. Please look into the matter and resolve at the earliest.\",\n",
    "                                  \"I want to open a salary account at your downtown branch. Please provide me the procedure.\",\n",
    "                                  \"Yesterday, I received a fraudulent email regarding renewal of my services.\",\n",
    "                                  \"What is the procedure to know my CIBIL score?\",\n",
    "                                  \"I need to know the number of bank branches and their locations in the city of Dubai\"]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
